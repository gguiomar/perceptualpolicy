Using device: cuda
buffer limit is =  100000
Episode: 0, total numsteps: 0, return: -0.2
Episode: 0, total numsteps: 0, return: -0.2
Episode: 0, total numsteps: 0, return: -0.2
Episode: 0, total numsteps: 0, return: -0.2
Episode: 0, total numsteps: 0, return: -0.2
Episode: 1, total numsteps: 50, return: -0.2
Cumulative Returns: -0.2
Episode: 2, total numsteps: 100, return: -0.2
Cumulative Returns: -0.4
Episode: 3, total numsteps: 150, return: -0.2
Cumulative Returns: -0.6000000000000001
Episode: 4, total numsteps: 200, return: -0.2
Cumulative Returns: -0.8
Episode: 5, total numsteps: 250, return: -0.2
Cumulative Returns: -1.0
Episode: 6, total numsteps: 300, return: -0.2
Cumulative Returns: -1.2
Episode: 7, total numsteps: 350, return: -0.2
Cumulative Returns: -1.4
Episode: 8, total numsteps: 400, return: -0.2
Cumulative Returns: -1.5999999999999999
Episode: 9, total numsteps: 450, return: -0.2
Cumulative Returns: -1.7999999999999998
Episode: 10, total numsteps: 500, return: -0.2
Cumulative Returns: -1.9999999999999998
Episode: 11, total numsteps: 550, return: -0.2
Cumulative Returns: -2.1999999999999997
Episode: 12, total numsteps: 600, return: -0.2
Cumulative Returns: -2.4
Episode: 13, total numsteps: 650, return: -0.2
Cumulative Returns: -2.6
Episode: 14, total numsteps: 700, return: -0.2
Cumulative Returns: -2.8000000000000003
Episode: 15, total numsteps: 750, return: -0.2
Cumulative Returns: -3.0000000000000004
Episode: 16, total numsteps: 800, return: -0.2
Cumulative Returns: -3.2000000000000006
Episode: 17, total numsteps: 850, return: -0.2
Cumulative Returns: -3.400000000000001
Episode: 18, total numsteps: 900, return: -0.2
Cumulative Returns: -3.600000000000001
Episode: 19, total numsteps: 950, return: -0.2
Cumulative Returns: -3.800000000000001
Episode: 20, total numsteps: 1000, return: -0.2
Cumulative Returns: -4.000000000000001
Episode: 21, total numsteps: 1050, return: -0.2
Cumulative Returns: -4.200000000000001
Episode: 22, total numsteps: 1100, return: -0.2
Cumulative Returns: -4.400000000000001
Episode: 23, total numsteps: 1150, return: -0.2
Cumulative Returns: -4.600000000000001
Episode: 24, total numsteps: 1200, return: -0.2
Cumulative Returns: -4.800000000000002
Episode: 25, total numsteps: 1250, return: -0.2
Cumulative Returns: -5.000000000000002
Episode: 26, total numsteps: 1300, return: -0.2
Cumulative Returns: -5.200000000000002
Episode: 27, total numsteps: 1350, return: -0.2
Cumulative Returns: -5.400000000000002
Episode: 28, total numsteps: 1400, return: -0.2
Cumulative Returns: -5.600000000000002
Episode: 29, total numsteps: 1450, return: -0.2
Cumulative Returns: -5.8000000000000025
Episode: 30, total numsteps: 1500, return: -0.2
Cumulative Returns: -6.000000000000003
Episode: 31, total numsteps: 1550, return: -0.2
Cumulative Returns: -6.200000000000003
Episode: 32, total numsteps: 1600, return: -0.2
Cumulative Returns: -6.400000000000003
Episode: 33, total numsteps: 1650, return: -0.2
Cumulative Returns: -6.600000000000003
Episode: 34, total numsteps: 1700, return: -0.2
Cumulative Returns: -6.800000000000003
Traceback (most recent call last):
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/main-alm.py", line 276, in <module>
    alm_helper.train()
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/main-alm.py", line 141, in train
    self.agent.update(self._train_step)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/alm.py", line 118, in update
    self.update_representation(std, step, log, metrics)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/alm.py", line 136, in update_representation
    alm_loss = self.alm_loss(state_seq, action_seq, next_state_seq, std, step, False, metrics)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/alm.py", line 156, in alm_loss
    kl_loss, z_next_prior_batch = self._kl_loss(z_batch, action_seq[t], next_state_seq[t], log, metrics)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/alm.py", line 170, in _kl_loss
    z_next_dist = self.encoder_target(next_state_batch)
  File "/home/themandalorian/anaconda3/envs/alm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/themandalorian/anaconda3/envs/alm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/alm_model.py", line 28, in forward
    return td.independent.Independent(td.Normal(mean, std), 1)
  File "/home/themandalorian/anaconda3/envs/alm/lib/python3.10/site-packages/torch/distributions/normal.py", line 59, in __init__
    super().__init__(batch_shape, validate_args=validate_args)
  File "/home/themandalorian/anaconda3/envs/alm/lib/python3.10/site-packages/torch/distributions/distribution.py", line 69, in __init__
    valid = constraint.check(value)
KeyboardInterrupt