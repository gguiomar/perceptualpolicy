Using device: cuda
buffer limit is =  100000
Episode: 0, total numsteps: 0, return: -1.0
Episode: 0, total numsteps: 0, return: -1.0
Episode: 0, total numsteps: 0, return: -1.0
Episode: 0, total numsteps: 0, return: -1.0
Episode: 0, total numsteps: 0, return: -1.0
Episode: 1, total numsteps: 100, return: -1.0
Cumulative Returns: -1.0
Episode: 2, total numsteps: 200, return: -1.0
Cumulative Returns: -2.0
Episode: 3, total numsteps: 300, return: -1.0
Cumulative Returns: -3.0
Episode: 4, total numsteps: 400, return: -1.0
Cumulative Returns: -4.0
Episode: 5, total numsteps: 500, return: -1.0
Cumulative Returns: -5.0
Episode: 6, total numsteps: 600, return: 75.0
Cumulative Returns: 70.0
Episode: 7, total numsteps: 700, return: -1.0
Cumulative Returns: 69.0
Episode: 8, total numsteps: 800, return: -1.0
Cumulative Returns: 68.0
Episode: 9, total numsteps: 900, return: -1.0
Cumulative Returns: 67.0
Episode: 10, total numsteps: 1000, return: -1.0
Cumulative Returns: 66.0
Episode: 11, total numsteps: 1100, return: -1.0
Cumulative Returns: 65.0
Episode: 12, total numsteps: 1200, return: -1.0
Cumulative Returns: 64.0
Episode: 13, total numsteps: 1300, return: -1.0
Cumulative Returns: 63.0
Episode: 14, total numsteps: 1400, return: -1.0
Cumulative Returns: 62.0
Episode: 15, total numsteps: 1500, return: -1.0
Cumulative Returns: 61.0
Episode: 16, total numsteps: 1600, return: -1.0
Cumulative Returns: 60.0
Episode: 17, total numsteps: 1700, return: -1.0
Cumulative Returns: 59.0
Episode: 18, total numsteps: 1800, return: -1.0
Cumulative Returns: 58.0
Traceback (most recent call last):
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/main-alm.py", line 274, in <module>
    alm_helper.train()
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/main-alm.py", line 141, in train
    self.agent.update(self._train_step)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/alm.py", line 119, in update
    self.update_rest(std, step, log, metrics)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/alm.py", line 223, in update_rest
    self.update_actor(z_dist.sample(), std, log, metrics)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/alm.py", line 298, in update_actor
    actor_loss.backward()
  File "/home/themandalorian/anaconda3/envs/alm/lib/python3.10/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/home/themandalorian/anaconda3/envs/alm/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/themandalorian/anaconda3/envs/alm/lib/python3.10/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt