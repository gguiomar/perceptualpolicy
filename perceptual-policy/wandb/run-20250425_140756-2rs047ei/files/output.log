Using device: cuda
buffer limit is =  100000
Episode: 0, total numsteps: 0, return: -0.25
Episode: 0, total numsteps: 0, return: -0.2
Episode: 0, total numsteps: 0, return: -0.2
Episode: 0, total numsteps: 0, return: -0.25
Episode: 0, total numsteps: 0, return: -0.05
Episode: 1, total numsteps: 50, return: -0.25
Cumulative Returns: -0.25
Episode: 2, total numsteps: 100, return: -0.1
Cumulative Returns: -0.35
Episode: 3, total numsteps: 150, return: -0.25
Cumulative Returns: -0.6
Episode: 4, total numsteps: 200, return: -0.25
Cumulative Returns: -0.85
Episode: 5, total numsteps: 250, return: -0.2
Cumulative Returns: -1.05
Episode: 6, total numsteps: 300, return: -0.2
Cumulative Returns: -1.25
Episode: 7, total numsteps: 350, return: -0.25
Cumulative Returns: -1.5
Episode: 8, total numsteps: 400, return: -0.25
Cumulative Returns: -1.75
Episode: 9, total numsteps: 450, return: -0.25
Cumulative Returns: -2.0
Episode: 10, total numsteps: 500, return: -0.25
Cumulative Returns: -2.25
Episode: 11, total numsteps: 550, return: -0.25
Cumulative Returns: -2.5
Episode: 12, total numsteps: 600, return: -0.25
Cumulative Returns: -2.75
Episode: 13, total numsteps: 650, return: -0.25
Cumulative Returns: -3.0
Episode: 14, total numsteps: 700, return: -0.25
Cumulative Returns: -3.25
Episode: 15, total numsteps: 750, return: -0.25
Cumulative Returns: -3.5
Episode: 16, total numsteps: 800, return: -0.25
Cumulative Returns: -3.75
Episode: 17, total numsteps: 850, return: -0.25
Cumulative Returns: -4.0
Episode: 18, total numsteps: 900, return: -0.25
Cumulative Returns: -4.25
Episode: 19, total numsteps: 950, return: -0.25
Cumulative Returns: -4.5
Episode: 20, total numsteps: 1000, return: -0.25
Cumulative Returns: -4.75
Episode: 21, total numsteps: 1050, return: -0.25
Cumulative Returns: -5.0
Episode: 22, total numsteps: 1100, return: -0.25
Cumulative Returns: -5.25
Episode: 23, total numsteps: 1150, return: -0.25
Cumulative Returns: -5.5
Episode: 24, total numsteps: 1200, return: -0.25
Cumulative Returns: -5.75
Episode: 25, total numsteps: 1250, return: -0.25
Cumulative Returns: -6.0
Episode: 26, total numsteps: 1300, return: -0.25
Cumulative Returns: -6.25
Episode: 27, total numsteps: 1350, return: -0.25
Cumulative Returns: -6.5
Episode: 28, total numsteps: 1400, return: -0.25
Cumulative Returns: -6.75
Episode: 29, total numsteps: 1450, return: -0.25
Cumulative Returns: -7.0
Episode: 30, total numsteps: 1500, return: -0.25
Cumulative Returns: -7.25
Episode: 31, total numsteps: 1550, return: -0.25
Cumulative Returns: -7.5
Episode: 32, total numsteps: 1600, return: -0.25
Cumulative Returns: -7.75
Episode: 33, total numsteps: 1650, return: -0.25
Cumulative Returns: -8.0
Episode: 34, total numsteps: 1700, return: -0.25
Cumulative Returns: -8.25
Episode: 35, total numsteps: 1750, return: -0.25
Cumulative Returns: -8.5
Episode: 36, total numsteps: 1800, return: -0.25
Cumulative Returns: -8.75
Episode: 37, total numsteps: 1850, return: -0.25
Cumulative Returns: -9.0
Episode: 38, total numsteps: 1900, return: -0.25
Cumulative Returns: -9.25
Episode: 39, total numsteps: 1950, return: -0.25
Cumulative Returns: -9.5
Episode: 40, total numsteps: 2000, return: -0.25
Cumulative Returns: -9.75
Episode: 41, total numsteps: 2050, return: -0.25
Cumulative Returns: -10.0
Episode: 42, total numsteps: 2100, return: -0.25
Cumulative Returns: -10.25
Episode: 43, total numsteps: 2150, return: -0.25
Cumulative Returns: -10.5
Episode: 44, total numsteps: 2200, return: -0.25
Cumulative Returns: -10.75
Episode: 45, total numsteps: 2250, return: -0.25
Cumulative Returns: -11.0
Episode: 46, total numsteps: 2300, return: -0.25
Cumulative Returns: -11.25
Episode: 47, total numsteps: 2350, return: -0.25
Cumulative Returns: -11.5
Episode: 48, total numsteps: 2400, return: -0.25
Cumulative Returns: -11.75
Episode: 49, total numsteps: 2450, return: -0.25
Cumulative Returns: -12.0
Episode: 50, total numsteps: 2500, return: -0.25
Cumulative Returns: -12.25
Episode: 51, total numsteps: 2550, return: -0.25
Cumulative Returns: -12.5
Episode: 52, total numsteps: 2600, return: -0.25
Cumulative Returns: -12.75
Episode: 53, total numsteps: 2650, return: -0.25
Cumulative Returns: -13.0
Episode: 54, total numsteps: 2700, return: -0.25
Cumulative Returns: -13.25
Episode: 55, total numsteps: 2750, return: -0.25
Cumulative Returns: -13.5
Episode: 56, total numsteps: 2800, return: -0.25
Cumulative Returns: -13.75
Episode: 57, total numsteps: 2850, return: -0.25
Cumulative Returns: -14.0
Episode: 58, total numsteps: 2900, return: -0.25
Cumulative Returns: -14.25
Episode: 59, total numsteps: 2950, return: -0.25
Cumulative Returns: -14.5
Episode: 60, total numsteps: 3000, return: -0.25
Cumulative Returns: -14.75
Episode: 61, total numsteps: 3050, return: -0.25
Cumulative Returns: -15.0
Episode: 62, total numsteps: 3100, return: -0.25
Cumulative Returns: -15.25
Episode: 63, total numsteps: 3150, return: -0.25
Cumulative Returns: -15.5
Traceback (most recent call last):
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/main-alm.py", line 276, in <module>
    alm_helper.train()
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/main-alm.py", line 141, in train
    self.agent.update(self._train_step)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/alm.py", line 118, in update
    self.update_representation(std, step, log, metrics)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/alm.py", line 129, in update_representation
    state_seq, action_seq, reward_seq, next_state_seq, done_seq = self.env_buffer.sample_seq(self.seq_len, self.batch_size)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/utils/replay_buffer.py", line 32, in sample_seq
    obs, act, rew, next_obs, term = self._retrieve_batch(np.asarray([self._sample_idx(l) for _ in range(n)]), n, l)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/utils/replay_buffer.py", line 32, in <listcomp>
    obs, act, rew, next_obs, term = self._retrieve_batch(np.asarray([self._sample_idx(l) for _ in range(n)]), n, l)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/utils/replay_buffer.py", line 42, in _sample_idx
    idx = np.random.randint(0, self.buffer_limit if self.full else self.idx-L)
KeyboardInterrupt