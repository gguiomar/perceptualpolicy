Using device: cuda
buffer limit is =  90000
Episode: 0, total numsteps: 0, return: 0.0
Episode: 0, total numsteps: 0, return: 0.0
Episode: 0, total numsteps: 0, return: 0.0
Episode: 0, total numsteps: 0, return: 0.0
Episode: 0, total numsteps: 0, return: 0.0
Episode: 1, total numsteps: 18, return: 0.0
Cumulative Returns: 0.0
Episode: 2, total numsteps: 36, return: 0.0
Cumulative Returns: 0.0
Episode: 3, total numsteps: 54, return: 0.0
Cumulative Returns: 0.0
Episode: 4, total numsteps: 72, return: 0.0
Cumulative Returns: 0.0
Episode: 5, total numsteps: 90, return: 0.0
Cumulative Returns: 0.0
Episode: 6, total numsteps: 108, return: 0.0
Cumulative Returns: 0.0
Episode: 7, total numsteps: 126, return: 0.0
Cumulative Returns: 0.0
Episode: 8, total numsteps: 144, return: 0.0
Cumulative Returns: 0.0
Episode: 9, total numsteps: 162, return: 0.0
Cumulative Returns: 0.0
Episode: 10, total numsteps: 180, return: 0.0
Cumulative Returns: 0.0
Episode: 11, total numsteps: 198, return: 0.0
Cumulative Returns: 0.0
Episode: 12, total numsteps: 216, return: 0.0
Cumulative Returns: 0.0
Episode: 13, total numsteps: 234, return: 0.0
Cumulative Returns: 0.0
Episode: 14, total numsteps: 252, return: 0.0
Cumulative Returns: 0.0
Episode: 15, total numsteps: 270, return: 0.0
Cumulative Returns: 0.0
Episode: 16, total numsteps: 288, return: 0.0
Cumulative Returns: 0.0
Episode: 17, total numsteps: 306, return: 0.0
Cumulative Returns: 0.0
Episode: 18, total numsteps: 324, return: 0.0
Cumulative Returns: 0.0
Episode: 19, total numsteps: 342, return: 0.0
Cumulative Returns: 0.0
Episode: 20, total numsteps: 360, return: 0.0
Cumulative Returns: 0.0
Episode: 21, total numsteps: 378, return: 0.0
Cumulative Returns: 0.0
Episode: 22, total numsteps: 396, return: 0.0
Cumulative Returns: 0.0
Episode: 23, total numsteps: 414, return: 0.0
Cumulative Returns: 0.0
Episode: 24, total numsteps: 432, return: 0.0
Cumulative Returns: 0.0
Episode: 25, total numsteps: 450, return: 0.0
Cumulative Returns: 0.0
Episode: 26, total numsteps: 468, return: 0.0
Cumulative Returns: 0.0
Episode: 27, total numsteps: 486, return: 0.0
Cumulative Returns: 0.0
Episode: 28, total numsteps: 504, return: 0.0
Cumulative Returns: 0.0
Episode: 29, total numsteps: 522, return: 0.0
Cumulative Returns: 0.0
Episode: 30, total numsteps: 540, return: 0.0
Cumulative Returns: 0.0
Episode: 31, total numsteps: 558, return: 0.0
Cumulative Returns: 0.0
Episode: 32, total numsteps: 576, return: 0.0
Cumulative Returns: 0.0
Episode: 33, total numsteps: 594, return: 0.0
Cumulative Returns: 0.0
Episode: 34, total numsteps: 612, return: 0.0
Cumulative Returns: 0.0
Episode: 35, total numsteps: 630, return: 0.0
Cumulative Returns: 0.0
Episode: 36, total numsteps: 648, return: 0.0
Cumulative Returns: 0.0
Episode: 37, total numsteps: 666, return: 0.0
Cumulative Returns: 0.0
Episode: 38, total numsteps: 684, return: 0.0
Cumulative Returns: 0.0
Episode: 39, total numsteps: 702, return: 0.0
Cumulative Returns: 0.0
Episode: 40, total numsteps: 720, return: 0.0
Cumulative Returns: 0.0
Episode: 41, total numsteps: 738, return: 0.0
Cumulative Returns: 0.0
Episode: 42, total numsteps: 756, return: 0.0
Cumulative Returns: 0.0
Episode: 43, total numsteps: 774, return: 0.0
Cumulative Returns: 0.0
Episode: 44, total numsteps: 792, return: 0.0
Cumulative Returns: 0.0
Episode: 45, total numsteps: 810, return: 0.0
Cumulative Returns: 0.0
Episode: 46, total numsteps: 828, return: 0.0
Cumulative Returns: 0.0
Episode: 47, total numsteps: 846, return: 0.0
Cumulative Returns: 0.0
Episode: 48, total numsteps: 864, return: 0.0
Cumulative Returns: 0.0
Episode: 49, total numsteps: 882, return: 0.0
Cumulative Returns: 0.0
Episode: 50, total numsteps: 900, return: 0.0
Cumulative Returns: 0.0
Episode: 51, total numsteps: 918, return: 0.0
Cumulative Returns: 0.0
Episode: 52, total numsteps: 936, return: 0.0
Cumulative Returns: 0.0
Episode: 53, total numsteps: 954, return: 0.0
Cumulative Returns: 0.0
Episode: 54, total numsteps: 972, return: 0.0
Cumulative Returns: 0.0
Episode: 55, total numsteps: 990, return: 0.0
Cumulative Returns: 0.0
Episode: 56, total numsteps: 1008, return: 0.0
Cumulative Returns: 0.0
Episode: 57, total numsteps: 1026, return: 0.0
Cumulative Returns: 0.0
Episode: 58, total numsteps: 1044, return: 0.0
Cumulative Returns: 0.0
Episode: 59, total numsteps: 1062, return: 0.0
Cumulative Returns: 0.0
Episode: 60, total numsteps: 1080, return: 0.0
Cumulative Returns: 0.0
Episode: 61, total numsteps: 1098, return: 0.0
Cumulative Returns: 0.0
Episode: 62, total numsteps: 1116, return: 0.0
Cumulative Returns: 0.0
Episode: 63, total numsteps: 1134, return: 0.0
Cumulative Returns: 0.0
Episode: 64, total numsteps: 1152, return: 0.0
Cumulative Returns: 0.0
Episode: 65, total numsteps: 1170, return: 0.0
Cumulative Returns: 0.0
Episode: 66, total numsteps: 1188, return: 0.0
Cumulative Returns: 0.0
Episode: 67, total numsteps: 1206, return: 0.0
Cumulative Returns: 0.0
Episode: 68, total numsteps: 1224, return: 0.0
Cumulative Returns: 0.0
Episode: 69, total numsteps: 1242, return: 0.0
Cumulative Returns: 0.0
Episode: 70, total numsteps: 1260, return: 0.0
Cumulative Returns: 0.0
Episode: 71, total numsteps: 1278, return: 0.0
Cumulative Returns: 0.0
Traceback (most recent call last):
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/main-alm.py", line 274, in <module>
    alm_helper.train()
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/main-alm.py", line 141, in train
    self.agent.update(self._train_step)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/alm.py", line 119, in update
    self.update_rest(std, step, log, metrics)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/alm.py", line 223, in update_rest
    self.update_actor(z_dist.sample(), std, log, metrics)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/alm.py", line 295, in update_actor
    actor_loss = self._lambda_svg_loss(z_batch, std, log, metrics)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/alm.py", line 317, in _lambda_svg_loss
    z_seq, action_seq = self._rollout_imagination(z_batch, std)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/alm.py", line 353, in _rollout_imagination
    action_batch = action_dist.sample(self.stddev_clip)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/utils/torch_utils.py", line 74, in sample
    return self._clamp(x)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/utils/torch_utils.py", line 62, in _clamp
    x = x - x.detach() + clamped_x.detach()
KeyboardInterrupt