Using device: cuda
buffer limit is =  100000
Episode: 0, total numsteps: 0, return: -0.2
Episode: 0, total numsteps: 0, return: -0.2
Episode: 0, total numsteps: 0, return: -0.2
Episode: 0, total numsteps: 0, return: -0.2
Episode: 0, total numsteps: 0, return: -0.2
Episode: 1, total numsteps: 25, return: -0.2
Cumulative Returns: -0.2
Episode: 2, total numsteps: 50, return: -0.2
Cumulative Returns: -0.4
Episode: 3, total numsteps: 75, return: -0.2
Cumulative Returns: -0.6000000000000001
Episode: 4, total numsteps: 100, return: -0.2
Cumulative Returns: -0.8
Episode: 5, total numsteps: 125, return: -0.2
Cumulative Returns: -1.0
Episode: 6, total numsteps: 150, return: -0.2
Cumulative Returns: -1.2
Episode: 7, total numsteps: 175, return: -0.2
Cumulative Returns: -1.4
Episode: 8, total numsteps: 200, return: -0.2
Cumulative Returns: -1.5999999999999999
Episode: 9, total numsteps: 225, return: -0.2
Cumulative Returns: -1.7999999999999998
Episode: 10, total numsteps: 250, return: -0.2
Cumulative Returns: -1.9999999999999998
Episode: 11, total numsteps: 275, return: -0.2
Cumulative Returns: -2.1999999999999997
Episode: 12, total numsteps: 300, return: -0.2
Cumulative Returns: -2.4
Episode: 13, total numsteps: 325, return: -0.2
Cumulative Returns: -2.6
Episode: 14, total numsteps: 350, return: -0.2
Cumulative Returns: -2.8000000000000003
Episode: 15, total numsteps: 375, return: -0.2
Cumulative Returns: -3.0000000000000004
Episode: 16, total numsteps: 400, return: -0.2
Cumulative Returns: -3.2000000000000006
Episode: 17, total numsteps: 425, return: -0.2
Cumulative Returns: -3.400000000000001
Episode: 18, total numsteps: 450, return: -0.2
Cumulative Returns: -3.600000000000001
Episode: 19, total numsteps: 475, return: -0.2
Cumulative Returns: -3.800000000000001
Episode: 20, total numsteps: 500, return: -0.2
Cumulative Returns: -4.000000000000001
Episode: 21, total numsteps: 525, return: -0.2
Cumulative Returns: -4.200000000000001
Episode: 22, total numsteps: 550, return: -0.2
Cumulative Returns: -4.400000000000001
Episode: 23, total numsteps: 575, return: -0.2
Cumulative Returns: -4.600000000000001
Episode: 24, total numsteps: 600, return: -0.2
Cumulative Returns: -4.800000000000002
Episode: 25, total numsteps: 625, return: -0.2
Cumulative Returns: -5.000000000000002
Episode: 26, total numsteps: 650, return: -0.2
Cumulative Returns: -5.200000000000002
Episode: 27, total numsteps: 675, return: -0.2
Cumulative Returns: -5.400000000000002
Episode: 28, total numsteps: 700, return: -0.2
Cumulative Returns: -5.600000000000002
Episode: 29, total numsteps: 725, return: -0.2
Cumulative Returns: -5.8000000000000025
Episode: 30, total numsteps: 750, return: -0.2
Cumulative Returns: -6.000000000000003
Episode: 31, total numsteps: 775, return: -0.2
Cumulative Returns: -6.200000000000003
Episode: 32, total numsteps: 800, return: -0.2
Cumulative Returns: -6.400000000000003
Episode: 33, total numsteps: 825, return: -0.2
Cumulative Returns: -6.600000000000003
Episode: 34, total numsteps: 850, return: -0.2
Cumulative Returns: -6.800000000000003
Episode: 35, total numsteps: 875, return: -0.2
Cumulative Returns: -7.0000000000000036
Episode: 36, total numsteps: 900, return: -0.2
Cumulative Returns: -7.200000000000004
Episode: 37, total numsteps: 925, return: -0.2
Cumulative Returns: -7.400000000000004
Episode: 38, total numsteps: 950, return: -0.2
Cumulative Returns: -7.600000000000004
Episode: 39, total numsteps: 975, return: -0.2
Cumulative Returns: -7.800000000000004
Episode: 40, total numsteps: 1000, return: -0.2
Cumulative Returns: -8.000000000000004
Episode: 41, total numsteps: 1025, return: -0.2
Cumulative Returns: -8.200000000000003
Episode: 42, total numsteps: 1050, return: -0.2
Cumulative Returns: -8.400000000000002
Episode: 43, total numsteps: 1075, return: -0.2
Cumulative Returns: -8.600000000000001
Episode: 44, total numsteps: 1100, return: -0.2
Cumulative Returns: -8.8
Episode: 45, total numsteps: 1125, return: -0.2
Cumulative Returns: -9.0
Episode: 46, total numsteps: 1150, return: -0.2
Cumulative Returns: -9.2
Episode: 47, total numsteps: 1175, return: -0.2
Cumulative Returns: -9.399999999999999
Episode: 48, total numsteps: 1200, return: -0.2
Cumulative Returns: -9.599999999999998
Episode: 49, total numsteps: 1225, return: -0.2
Cumulative Returns: -9.799999999999997
Episode: 50, total numsteps: 1250, return: -0.2
Cumulative Returns: -9.999999999999996
Episode: 51, total numsteps: 1275, return: -0.2
Cumulative Returns: -10.199999999999996
Episode: 52, total numsteps: 1300, return: -0.2
Cumulative Returns: -10.399999999999995
Episode: 53, total numsteps: 1325, return: -0.2
Cumulative Returns: -10.599999999999994
Episode: 54, total numsteps: 1350, return: -0.2
Cumulative Returns: -10.799999999999994
Episode: 55, total numsteps: 1375, return: -0.2
Cumulative Returns: -10.999999999999993
Episode: 56, total numsteps: 1400, return: -0.2
Cumulative Returns: -11.199999999999992
Episode: 57, total numsteps: 1425, return: -0.2
Cumulative Returns: -11.399999999999991
Episode: 58, total numsteps: 1450, return: -0.2
Cumulative Returns: -11.59999999999999
Episode: 59, total numsteps: 1475, return: -0.2
Cumulative Returns: -11.79999999999999
Episode: 60, total numsteps: 1500, return: -0.2
Cumulative Returns: -11.99999999999999
Episode: 61, total numsteps: 1525, return: -0.2
Cumulative Returns: -12.199999999999989
Episode: 62, total numsteps: 1550, return: -0.2
Cumulative Returns: -12.399999999999988
Episode: 63, total numsteps: 1575, return: -0.2
Cumulative Returns: -12.599999999999987
Episode: 64, total numsteps: 1600, return: -0.2
Cumulative Returns: -12.799999999999986
Episode: 65, total numsteps: 1625, return: -0.2
Cumulative Returns: -12.999999999999986
Traceback (most recent call last):
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/main-alm.py", line 276, in <module>
    alm_helper.train()
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/main-alm.py", line 141, in train
    self.agent.update(self._train_step)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/alm.py", line 119, in update
    self.update_rest(std, step, log, metrics)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/alm.py", line 223, in update_rest
    self.update_actor(z_dist.sample(), std, log, metrics)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/alm.py", line 295, in update_actor
    actor_loss = self._lambda_svg_loss(z_batch, std, log, metrics)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/alm.py", line 317, in _lambda_svg_loss
    z_seq, action_seq = self._rollout_imagination(z_batch, std)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/alm.py", line 354, in _rollout_imagination
    z_batch = self.model(z_batch, action_batch).rsample()
  File "/home/themandalorian/anaconda3/envs/alm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/themandalorian/anaconda3/envs/alm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/themandalorian/ETH/Thesis/perceptualpolicy/alm_model.py", line 58, in forward
    return td.independent.Independent(td.Normal(mean, std), 1)
  File "/home/themandalorian/anaconda3/envs/alm/lib/python3.10/site-packages/torch/distributions/normal.py", line 59, in __init__
    super().__init__(batch_shape, validate_args=validate_args)
  File "/home/themandalorian/anaconda3/envs/alm/lib/python3.10/site-packages/torch/distributions/distribution.py", line 70, in __init__
    if not valid.all():
KeyboardInterrupt