{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Avoidance Task Visualization Notebook\n",
    "\n",
    "This notebook allows you to train an agent for the active avoidance task and then visualize its performance without having to retrain the agent each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Import the environment\n",
    "from envs.active_avoidance_env_test import ActiveAvoidanceEnv2D\n",
    "\n",
    "# Import agents\n",
    "from agents.ppo_agent import PPOAgent\n",
    "from agents.maxent_agent import MaxEntAgent, FisherMaxEntAgent\n",
    "from agents.trpo_agent import TRPOAgent\n",
    "\n",
    "# Import the visualization functions\n",
    "from utils.avoidance_visualization import (\n",
    "    plot_avoidance_training_curves,\n",
    "    plot_avoidance_trajectories_2d,\n",
    "    plot_avoidance_heatmap_2d,\n",
    "    plot_avoidance_trajectory_step_by_step,\n",
    "    plot_multiple_avoidance_trajectories\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Define your configuration parameters here. You can modify these to experiment with different settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  height: 10\n",
      "  width: 20\n",
      "  tone_duration_steps: 30\n",
      "  shock_delay_steps: 30\n",
      "  max_steps_per_episode: 100\n",
      "  initial_task: 1\n",
      "  agent_class: MaxEntAgent\n",
      "  agent_name: MaxEnt_RNN\n",
      "  policy_type: rnn\n",
      "  hidden_dim: 128\n",
      "  lr: 0.001\n",
      "  gamma: 0.993\n",
      "  temperature: 0.15\n",
      "  rnn_type: gru\n",
      "  num_episodes: 2000\n",
      "  task_switch_episode: 1000\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Ensure plots directory exists\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "# --- Configuration ---\n",
    "config = {\n",
    "    # Environment params\n",
    "    'height': 10,\n",
    "    'width': 20,\n",
    "    'tone_duration_steps': 30,\n",
    "    'shock_delay_steps': 30,\n",
    "    'max_steps_per_episode': 100,\n",
    "    'initial_task': ActiveAvoidanceEnv2D.AVOID_TONE_1,\n",
    "\n",
    "    # Agent params\n",
    "    'agent_class': MaxEntAgent, \n",
    "    'agent_name': 'MaxEnt_RNN', \n",
    "    'policy_type': 'rnn',       \n",
    "    'hidden_dim': 128,           # Hidden dimension for RNN\n",
    "    # 'hidden_dims': [128, 128], # multi-layer MLP - comment out for rnn\n",
    "    'lr': 0.001,                \n",
    "    'gamma': 0.993,\n",
    "    # 'epsilon': 0.2,           # PPO specific\n",
    "    'temperature': 0.15,         # MaxEnt specific\n",
    "    # 'kl_delta': 0.01,         # TRPO specific\n",
    "    'rnn_type': 'gru',          # Specify RNN type ('gru' or 'lstm' or 'rnn')\n",
    "\n",
    "    # Training params\n",
    "    'num_episodes': 2000,\n",
    "    'task_switch_episode': 1000\n",
    "}\n",
    "\n",
    "# Display configuration\n",
    "print(\"Configuration:\")\n",
    "for key, value in config.items():\n",
    "    if key == 'agent_class':\n",
    "        print(f\"  {key}: {value.__name__}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Environment and Agent\n",
    "\n",
    "Initialize the environment and agent based on the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Created MaxEnt_RNN with rnn policy.\n",
      "State dim: 5, Action dim: 5\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "env = ActiveAvoidanceEnv2D(\n",
    "    height=config['height'],\n",
    "    width=config['width'],\n",
    "    tone_duration_steps=config['tone_duration_steps'],\n",
    "    shock_delay_steps=config['shock_delay_steps'],\n",
    "    max_steps_per_episode=config['max_steps_per_episode'],\n",
    "    initial_task=config['initial_task']\n",
    ")\n",
    "\n",
    "state_dim = env.observation_space.shape[0] # Should be 4\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# --- Create Agent ---\n",
    "AgentClass = config['agent_class']\n",
    "\n",
    "# Construct agent parameters\n",
    "agent_params = {\n",
    "    'policy_type': config['policy_type'],\n",
    "    'state_dim': state_dim,\n",
    "    'action_dim': action_dim,\n",
    "    'lr': config['lr'],\n",
    "    'gamma': config['gamma']\n",
    "}\n",
    "if config['policy_type'] == 'mlp':\n",
    "    agent_params['hidden_dims'] = config.get('hidden_dims', [128, 128])\n",
    "elif config['policy_type'] in ['rnn', 'transformer']:\n",
    "    agent_params['hidden_dim'] = config['hidden_dim']\n",
    "    if config['policy_type'] == 'rnn':\n",
    "         agent_params['rnn_type'] = config.get('rnn_type', 'gru')\n",
    "\n",
    "# Add algorithm-specific params\n",
    "if AgentClass == PPOAgent:\n",
    "    agent_params['epsilon'] = config['epsilon']\n",
    "elif AgentClass in [MaxEntAgent, FisherMaxEntAgent]:\n",
    "     agent_params['temperature'] = config['temperature']\n",
    "     # Add Fisher specific params if needed for FisherMaxEntAgent\n",
    "elif AgentClass == TRPOAgent:\n",
    "     agent_params['kl_delta'] = config['kl_delta']\n",
    "     # Add TRPO specific params if needed\n",
    "\n",
    "# Instantiate the agent\n",
    "agent = AgentClass(**agent_params)\n",
    "\n",
    "print(f\"Created {config['agent_name']} with {config['policy_type']} policy.\")\n",
    "print(f\"State dim: {state_dim}, Action dim: {action_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Agent\n",
    "\n",
    "Train the agent and save the training history and model. You only need to run this cell once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True if you want to train the agent\n",
    "TRAIN_AGENT = True\n",
    "\n",
    "# Set to True if you want to load a previously trained agent\n",
    "LOAD_AGENT = False\n",
    "\n",
    "# Path to save/load the agent and training history\n",
    "agent_save_path = f\"saved_models/{config['agent_name']}_agent.pkl\"\n",
    "history_save_path = f\"saved_models/{config['agent_name']}_history.pkl\"\n",
    "\n",
    "if LOAD_AGENT and os.path.exists(agent_save_path) and os.path.exists(history_save_path):\n",
    "    # Load the agent and training history\n",
    "    print(f\"Loading agent from {agent_save_path}...\")\n",
    "    with open(agent_save_path, 'rb') as f:\n",
    "        agent = pickle.load(f)\n",
    "    \n",
    "    print(f\"Loading training history from {history_save_path}...\")\n",
    "    with open(history_save_path, 'rb') as f:\n",
    "        training_history = pickle.load(f)\n",
    "    \n",
    "    rewards_history = training_history['rewards']\n",
    "    losses_history = training_history['losses']\n",
    "    metric_history = training_history['metrics']\n",
    "    avoidance_rate_history = training_history['avoidance_rates']\n",
    "    shock_rate_history = training_history['shock_rates']\n",
    "    \n",
    "    print(\"Agent and training history loaded successfully.\")\n",
    "    \n",
    "elif TRAIN_AGENT:\n",
    "    # --- Training ---\n",
    "    print(\"Starting training...\")\n",
    "    rewards_history = []\n",
    "    losses_history = []\n",
    "    metric_history = []\n",
    "    avoidance_rate_history = []\n",
    "    shock_rate_history = []\n",
    "\n",
    "    for episode in range(config['num_episodes']):\n",
    "        # Task Switching Logic\n",
    "        if 'task_switch_episode' in config and episode == config['task_switch_episode']:\n",
    "            print(f\"\\n--- Switching Task at Episode {episode} ---\")\n",
    "            env.switch_task()\n",
    "\n",
    "        # Run Episode\n",
    "        state = env.reset()\n",
    "        states, actions, rewards, log_probs_old_list = [], [], [], []\n",
    "        hidden_states_list = []\n",
    "        episode_reward = 0\n",
    "        ep_info = {'avoided': False, 'shocked': False}\n",
    "\n",
    "        hidden_state = None\n",
    "        if config['policy_type'] in [\"rnn\"] and hasattr(agent.policy_net, 'init_hidden'):\n",
    "             hidden_state = agent.policy_net.init_hidden().to(device)\n",
    "\n",
    "        for step in range(config['max_steps_per_episode']):\n",
    "            # Select action\n",
    "            if config['policy_type'] in [\"rnn\"]:\n",
    "                action, log_prob, _, h_new = agent.select_action(state, hidden_state)\n",
    "                hidden_states_list.append(hidden_state)\n",
    "                hidden_state = h_new\n",
    "            else:\n",
    "                action, log_prob, _ = agent.select_action(state)\n",
    "\n",
    "            # Step environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Store transition\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs_old_list.append(log_prob) # Used by PPO/TRPO, ignored by MaxEnt update\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                ep_info = info\n",
    "                break\n",
    "\n",
    "        # Agent Update\n",
    "        update_args = [states, actions, rewards, log_probs_old_list] # MaxEnt ignores log_probs_old_list\n",
    "        if config['policy_type'] in [\"rnn\"]:\n",
    "             update_args.append(hidden_states_list)\n",
    "\n",
    "        update_info = agent.update(*update_args)\n",
    "\n",
    "        # Record History\n",
    "        rewards_history.append(episode_reward)\n",
    "        losses_history.append(update_info.get('loss', update_info.get('policy_loss', 0)))\n",
    "        # MaxEnt returns 'entropy', PPO/TRPO return 'approx_kl' or 'kl'\n",
    "        metric_history.append(update_info.get('approx_kl', update_info.get('entropy', update_info.get('kl', 0))))\n",
    "        avoidance_rate_history.append(1 if ep_info['avoided'] else 0)\n",
    "        shock_rate_history.append(1 if ep_info['shocked'] else 0)\n",
    "\n",
    "        # Print Progress\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(rewards_history[-50:])\n",
    "            avg_loss = np.mean(losses_history[-50:])\n",
    "            avg_metric = np.mean(metric_history[-50:])\n",
    "            avg_avoid = np.mean(avoidance_rate_history[-50:]) * 100\n",
    "            avg_shock = np.mean(shock_rate_history[-50:]) * 100\n",
    "\n",
    "            metric_name = 'Entropy' if isinstance(agent, (MaxEntAgent, FisherMaxEntAgent)) else 'KL'\n",
    "            print(f\"Ep {episode+1}/{config['num_episodes']} | Task: {'Tone1' if env.current_task_id == 1 else 'Tone2'} | \"\n",
    "                  f\"Rwd: {avg_reward:.2f} | Loss: {avg_loss:.4f} | {metric_name}: {avg_metric:.4f} | \"\n",
    "                  f\"Avoid%: {avg_avoid:.1f} | Shock%: {avg_shock:.1f}\")\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "    \n",
    "    # Save the agent and training history\n",
    "    print(f\"Saving agent to {agent_save_path}...\")\n",
    "    with open(agent_save_path, 'wb') as f:\n",
    "        pickle.dump(agent, f)\n",
    "    \n",
    "    print(f\"Saving training history to {history_save_path}...\")\n",
    "    training_history = {\n",
    "        'rewards': rewards_history,\n",
    "        'losses': losses_history,\n",
    "        'metrics': metric_history,\n",
    "        'avoidance_rates': avoidance_rate_history,\n",
    "        'shock_rates': shock_rate_history\n",
    "    }\n",
    "    with open(history_save_path, 'wb') as f:\n",
    "        pickle.dump(training_history, f)\n",
    "    \n",
    "    print(\"Agent and training history saved successfully.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping training. Set TRAIN_AGENT=True to train the agent or LOAD_AGENT=True to load a previously trained agent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot Training Curves\n",
    "\n",
    "Visualize the training performance of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have training history\n",
    "if 'rewards_history' in locals() and 'losses_history' in locals() and 'metric_history' in locals() and \\\n",
    "   'avoidance_rate_history' in locals() and 'shock_rate_history' in locals():\n",
    "    \n",
    "    metric_name = 'Entropy' if isinstance(agent, (MaxEntAgent, FisherMaxEntAgent)) else 'KL'\n",
    "    \n",
    "    plot_avoidance_training_curves(\n",
    "        rewards=rewards_history,\n",
    "        losses=losses_history,\n",
    "        metrics=metric_history,\n",
    "        metric_name=metric_name,\n",
    "        avoidance_rates=avoidance_rate_history,\n",
    "        shock_rates=shock_rate_history,\n",
    "        smooth_window=50,\n",
    "        save_path=f\"plots/{config['agent_name']}_training_curves.png\",\n",
    "    )\n",
    "    \n",
    "    print(f\"Training curves saved to plots/{config['agent_name']}_training_curves.png\")\n",
    "else:\n",
    "    print(\"No training history available. Please train the agent or load a previously trained agent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization Options\n",
    "\n",
    "Choose which visualization you want to generate. You can run these cells multiple times with different parameters without retraining the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Plot 2D Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for 2D trajectory visualization\n",
    "num_trajectories = 8\n",
    "max_steps = config['max_steps_per_episode']\n",
    "save_path = f\"plots/{config['agent_name']}_trajectory_task{env.current_task_id}.png\"\n",
    "\n",
    "try:\n",
    "    plot_avoidance_trajectories_2d(env, agent, num_trajectories=num_trajectories, max_steps=max_steps, save_path=save_path)\n",
    "    print(f\"2D trajectories saved to {save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate trajectory plot: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Plot Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for heatmap visualization\n",
    "num_episodes = 100\n",
    "max_steps = config['max_steps_per_episode']\n",
    "save_path = f\"plots/{config['agent_name']}_heatmap_task{env.current_task_id}.png\"\n",
    "\n",
    "try:\n",
    "    plot_avoidance_heatmap_2d(env, agent, num_episodes=num_episodes, max_steps=max_steps, save_path=save_path)\n",
    "    print(f\"Heatmap saved to {save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate heatmap plot: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Plot Step-by-Step Trajectory Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for step-by-step trajectory animation\n",
    "max_steps = config['max_steps_per_episode']\n",
    "save_path = f\"plots/{config['agent_name']}_step_by_step_task{env.current_task_id}.gif\"\n",
    "visualization_type = 'animation'  # 'animation' or 'gallery'\n",
    "fps = 5\n",
    "gallery_save_dir = None  # Set to a directory path if you want to save individual frames\n",
    "\n",
    "try:\n",
    "    plot_avoidance_trajectory_step_by_step(\n",
    "        env, agent, max_steps=max_steps, save_path=save_path, \n",
    "        visualization_type=visualization_type, fps=fps, gallery_save_dir=gallery_save_dir\n",
    "    )\n",
    "    print(f\"Step-by-step trajectory animation saved to {save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate step-by-step trajectory animation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Plot Multiple Trajectories Side by Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for multiple trajectories visualization\n",
    "num_runs = 4\n",
    "max_steps = config['max_steps_per_episode']\n",
    "save_path = f\"plots/{config['agent_name']}_multiple_trajectories_task{env.current_task_id}.gif\"\n",
    "visualization_type = 'animation'  # 'animation' or 'gallery'\n",
    "fps = 5\n",
    "gallery_save_dir = None  # Set to a directory path if you want to save individual frames\n",
    "random_seed = 42  # Set to None to use the environment's default randomness\n",
    "\n",
    "try:\n",
    "    plot_multiple_avoidance_trajectories(\n",
    "        env, agent, num_runs=num_runs, max_steps=max_steps, save_path=save_path, \n",
    "        visualization_type=visualization_type, fps=fps, gallery_save_dir=gallery_save_dir,\n",
    "        random_seed=random_seed\n",
    "    )\n",
    "    print(f\"Multiple trajectories animation saved to {save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate multiple trajectories animation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Experiment with Different Parameters\n",
    "\n",
    "You can modify the parameters in the visualization cells above to experiment with different settings without retraining the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Try different numbers of runs for multiple trajectories\n",
    "num_runs_options = [2, 4, 6, 8]\n",
    "\n",
    "for num_runs in num_runs_options:\n",
    "    save_path = f\"plots/{config['agent_name']}_multiple_trajectories_{num_runs}_runs_task{env.current_task_id}.gif\"\n",
    "    \n",
    "    try:\n",
    "        plot_multiple_avoidance_trajectories(\n",
    "            env, agent, num_runs=num_runs, max_steps=max_steps, save_path=save_path, \n",
    "            visualization_type='animation', fps=5, random_seed=42\n",
    "        )\n",
    "        print(f\"Multiple trajectories animation with {num_runs} runs saved to {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate multiple trajectories animation with {num_runs} runs: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Switch Task and Visualize\n",
    "\n",
    "You can switch the task and visualize the agent's performance on the new task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch task\n",
    "env.switch_task()\n",
    "print(f\"Switched to Task {'Tone1' if env.current_task_id == 1 else 'Tone2'}\")\n",
    "\n",
    "# Visualize the agent's performance on the new task\n",
    "save_path = f\"plots/{config['agent_name']}_multiple_trajectories_task{env.current_task_id}.gif\"\n",
    "\n",
    "try:\n",
    "    plot_multiple_avoidance_trajectories(\n",
    "        env, agent, num_runs=4, max_steps=max_steps, save_path=save_path, \n",
    "        visualization_type='animation', fps=5, random_seed=42\n",
    "    )\n",
    "    print(f\"Multiple trajectories animation for the new task saved to {save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate multiple trajectories animation for the new task: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
